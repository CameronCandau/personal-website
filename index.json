[{"content":"","date":"14 December 2025","externalUrl":null,"permalink":"/","section":"Cameron Candau","summary":"","title":"Cameron Candau","type":"page"},{"content":"","date":"14 December 2025","externalUrl":null,"permalink":"/series/homelab-v2/","section":"Series","summary":"","title":"Homelab V2","type":"series"},{"content":"","date":"14 December 2025","externalUrl":null,"permalink":"/homelab-v2/","section":"Homelab-V2s","summary":"","title":"Homelab-V2s","type":"homelab-v2"},{"content":"After my initial homelab infrastructure failed following a power outage, I rebuilt from scratch using infrastructure-as-code principles. This series documents the architecture, security decisions, and automation patterns I used to create production-grade infrastructure on consumer hardware.\nGuiding Principles # Make the Software Smart # My distributed systems course in university taught me the principle that even with unreliable or minimal hardware, we can create robust systems by making the software smart. Further, security doesn\u0026rsquo;t require an enterprise budget, just thoughtful design, and an enterprise budget doesn\u0026rsquo;t guarantee security either.\nPets vs Cattle # Goal: Build reproducible, secure homelab infrastructure\nv1 of my homelab setup was functional but less than ideal in many ways; overall my complaints boil down to the pets vs cattle argument in DevOps and cloud infrastructure.\nEach server was a special pet which required manual configuration, care, and documentation. Servers were difficult to replace because of this, and as a result, my infrastructure would be difficult to scale and reproduce.\nInfrastructure-as-Code solves this problem by declaring the configuration of infrastructure resources in a stateless configuration file. Servers become dispensable cattle which are generic and easy to replace with an identical copy that serves the same purpose.\nPets vs cattle.\nMy goal is to build reproducible, secure infrastructure. Tools such as Terraform, Ansible, and Docker will be the most important components in constructing such a lab.\nI\u0026rsquo;m still using the same hardware from version 1, at least to begin with.\nI also don\u0026rsquo;t have a managed switch for now, so I\u0026rsquo;m going to use a separate routed subnet (192.168.100.0/24) to create some level of separation, even if for nothing but organization at first, since this doesn\u0026rsquo;t actually create isolation.\nLastly, as with my previous setup, I want nightly cloud backups of application data, for disaster recovery purposes.\nArchitectural Decisions # Networking (Architecture \u0026amp; VPN) # I don\u0026rsquo;t have a managed switch right now so I can\u0026rsquo;t create proper network isolation with VLANs. In lieu of that, I\u0026rsquo;m going to be assigning my VMs static IP addresses in the 192.168.100.0/24 subnet.\nInternet → ISP Router (192.168.1.1) ↓ Proxmox VE 9.1 (192.168.1.10) ┌────────────────────────────────┐ │ vmbr0: 192.168.1.10 (external) │ │ vmbr1: 192.168.100.1 (VMs) │ │ ↓ NAT + Routing │ ├────────────────────────────────┤ │ VM Network: 192.168.100.0/24 │ │ ├─ Nextcloud │ │ ├─ Immich │ │ ├─ Jellyfin │ │ ├─ DNS │ │ └─ Traefik │ └────────────────────────────────┘ For VPN, I\u0026rsquo;ll install Tailscale on one VM to act a router to the entire subnet, meaning I\u0026rsquo;ll be able to reach all VMs while on VPN without installing the Tailscale client on each one individually.\nLater I plan to get granular with firewall rules to isolate this subnet more, and only allow traffic being routed through VPN.\nFurther, I\u0026rsquo;d also like to follow principles of zero-trust networking as much as possible, for instance, creating firewall rules to allow traffic between a guest and the router, but not between other guest VMs.\nThese ideas are both meant to limit potential for pivoting and lateral movement within the network. If an adversary gains access to one server, they can attempt to use that to enumerate the internal network as well. Although unlikely in a well-maintained homelab with limited access to begin with, this is much more important in enterprise networks which have a greater attack surface.\nBackups # To achieve efficient incremental backups, each VM operating a service will store application data on a (virtual) drive separate from the main OS. Then, all the data on this drive will get backed up to the cloud. This configuration will be consistent across every VM, making it very simple and preventing the need to manually configure the backup path for each application\u0026rsquo;s directory structure.\nDiffering from my previous setup however, I want to move to an agent-based backup system. Previously, I used a cron job on the proxmox host to orchestrate backing up each VM\u0026rsquo;s application drive as required. This new system will use an Ansible playbook to install the necessary tools and scripts on each guest, and they will back up their application data individually. This approach makes it trivial to scale to more VMs by simply running the playbook instead of manually editing the centralized script to include them.\nI made sure to test data recovery on a per-service basis after changing to this setup, as backups are only useful if you can restore them.\nConsistent with my previous setup, I\u0026rsquo;m going to continue using Restic to back up data to Backblaze B2. Using Backblaze was just an absolute breeze previously and I had already looked into other options pretty thoroughly, so I don\u0026rsquo;t feel any need to change this. B2 is are extremely affordable, and Restic makes it trivial to encrypt my data on the client side to provide assurance that my data is kept confidential while in Backblaze, even in the event that Backblaze\u0026rsquo;s servers are compromised.\nI also just like that this approach allows me to gain exposure to working with S3-compatible bucket storage; it\u0026rsquo;s more professional (and cheaper) than using consumer file storage like Google Drive.\nDNS \u0026amp; Privacy-Conscious Design # I\u0026rsquo;m using BIND9 for internal DNS resolution rather than Pi-hole. While Pi-hole is popular in homelab circles, BIND9 provides enterprise-grade DNS capabilities and better aligns with production infrastructure patterns.\nFor privacy and ad-blocking, I configured DNS over TLS (DoT) forwarding to Mullvad\u0026rsquo;s encrypted resolvers. This encrypts all external DNS queries from my infrastructure, preventing ISP monitoring while providing content filtering. Internal queries (.domain.local) resolve locally, while external queries forward over encrypted TLS connections.\nRunning an internal DNS server gives better privacy, as a public lookup of my domain gives no insight into my network architecture or the services I run.\nReverse Proxy # Previously I used NGINX Proxy Manager, which met my needs, but I wanted to try something more modern better aligned with infrastructure-as-code practices. I\u0026rsquo;ve seen Traefik recommended for these reasons, so I decided to give it a try.\nAs with my previous iteration, I\u0026rsquo;m still going to use Cloudflare\u0026rsquo;s DNS-01 challenge to issue a wildcard certificate for my services. I found this approach to be very convenient for my needs; I don\u0026rsquo;t need to publicly expose my server for LetsEncrypt to reach it (HTTP-01) so I can keep all services behind my LAN/VPN. Using a certificate from LetsEncrypt also means I don\u0026rsquo;t have to deal with the downsides of self-signed certificates, namely the need to import them into the trusted certificate store on each client.\nSecure Design Practices # This setup gives me defense in depth in multiple ways.\nWe have defense in depth shown in network design; no services are publicly exposed, each server will have a firewall configured for its exact needs, and each service will be kept up to date and configured with strong access management.\nSystems store application data separate from the OS; if a VM goes down, the disk still remains and can be re-attached to a freshly created VM.\nLastly, I heavily rely on environment variables. I can take down my infrastructure, rotate all credentials and the domain, and re-deploy without touching my actual infrastructure code. If I eventually publicize my whole project repo, I can be certain that my source code won\u0026rsquo;t leak secrets. The environment variable approach proved valuable during testing when Let\u0026rsquo;s Encrypt rate limits required switching to a backup domain—a simple variable change rather than code refactoring.\nSecrets are stored securely on my workstation (full-disk encryption) and in my cloud password manager. If I somehow lose my workstation, they still live in my password manager, ensuring I don\u0026rsquo;t lose access to my systems and data.\nComing Next # This post covered the architectural foundation. In upcoming posts, I\u0026rsquo;ll dive into:\nTerraform and Ansible implementation details VPN Router configuration DNS Server Deployment Automated TLS certificate management with DNS-01 challenges Encrypted backup automation with Restic ","date":"14 December 2025","externalUrl":null,"permalink":"/homelab-v2/infrastructure-architecture/","section":"Homelab-V2s","summary":"","title":"Infrastructure Architecture","type":"homelab-v2"},{"content":"","date":"14 December 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"I\u0026rsquo;ll be reworking my proxy setup, as originally I installed one directly on the Immich Installation server. Instead, we want one central reverse proxy which coordinates traffic between all of my application servers.\n192.168.100.3\nFollow docker instructions to get the latest version available for Ubuntu\u0026hellip; the apt repos are a bit outdated for Immich\u0026rsquo;s provided docker-compose configuration. https://docs.docker.com/engine/install/ubuntu/\nfor pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n# Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nTest installation sudo docker run hello-world\nhttps://youtu.be/Y7Z-RnM77tA?si=jt-6LJ8NDZaq6MxU\nI\u0026rsquo;ll make a new container on 192.168.100.10, immich, to be our reverse proxy with https://nginxproxymanager.com/guide/#quick-setup\nCreate account on cloudflare, add domain, change nameservers to Cloudflare\u0026rsquo;s if not already Get API token for editing DNS on your zone, save in password manager for future certs Paste into Nginx Proxy Manager config under SSL tab. Now we need to make a record to tell Cloudflare where to point when we visit immich.MYDOMAIN, which will be our Nginx server\u0026rsquo;s internal IP.\nThis tells cloudflare to resolve any subdomain of my domain to this internal IP. On this internal IP, we\u0026rsquo;ll reach Nginx Proxy Manager, which we\u0026rsquo;ve configured to proxy Immich for request to immich.MYDOMAIN\nNow we have a valid SSL certificate and hostname!\nIf I try visiting immich.MYDOMAIN from outside my home network, like my phone which isn\u0026rsquo;t on my home network, it will resolve but not be reachable\u0026hellip;. UNTIL I enable my Tailscale VPN client.\nNow I can download the Immich app from the app store and connect to my instance using this HTTPS URL!\nRepeat for other services. So far, just Jellyfin and this nginx proxy manager:\n","date":"16 August 2025","externalUrl":null,"permalink":"/homelab-v1/centralized-reverse-proxy/","section":"Homelab-V1s","summary":"","title":"Centralized Reverse Proxy","type":"homelab-v1"},{"content":"","date":"16 August 2025","externalUrl":null,"permalink":"/series/homelab-v1/","section":"Series","summary":"","title":"Homelab V1","type":"series"},{"content":"","date":"16 August 2025","externalUrl":null,"permalink":"/homelab-v1/","section":"Homelab-V1s","summary":"","title":"Homelab-V1s","type":"homelab-v1"},{"content":"New Ubuntu VM:\n40GB Storage 2 CPU cores 4096 MB RAM 150GB attached LVM disk Networking during installation: IP: 192.168.100.11 Subnet mask: 192.168.100.0/24 Gateway: 192.168.100.1\nAttach storage, as we did in Immich Installation.\n(Reboot)\nlsblk\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 40G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 38G 0 part └─ubuntu--vg-ubuntu--lv 252:0 0 19G 0 lvm / sdb 8:16 0 150G 0 disk sr0 11:0 1 3G 0 rom sudo fdisk /dev/sdb\nCommand (m for help): n Partition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions) Select (default p): p Partition number (1-4, default 1): First sector (2048-314572799, default 2048): Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-314572799, default 314572799): Created a new partition 1 of type \u0026#39;Linux\u0026#39; and of size 150 GiB. Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. lsblk\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 40G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 38G 0 part └─ubuntu--vg-ubuntu--lv 252:0 0 19G 0 lvm / sdb 8:16 0 150G 0 disk └─sdb1 8:17 0 150G 0 part sr0 11:0 1 3G 0 rom sudo mkdir -p /mnt/jellyfin-data \u0026amp;\u0026amp; sudo mount /dev/sdb1 /mnt/jellyfin-data/\ndf -h /mnt/media/\nFilesystem Size Used Avail Use% Mounted on /dev/sdb1 147G 28K 140G 1% /mnt/media sudo bklid /dev/sdb1\n/dev/sdb1: UUID=\u0026#34;0c9db7f3-d899-423d-ac0d-4314019a1b29\u0026#34; BLOCK_SIZE=\u0026#34;4096\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;ae2cf8da-01\u0026#34; Append into /etc/fstab: UUID=0c9db7f3-d899-423d-ac0d-4314019a1b29 /mnt/media ext4 defaults 0 2\nUnmount/remount\nsudo umount /mnt/media sudo systemctl daemon-reload sudo mount -a lsblk\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 40G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 38G 0 part └─ubuntu--vg-ubuntu--lv 252:0 0 19G 0 lvm / sdb 8:16 0 150G 0 disk └─sdb1 8:17 0 150G 0 part /mnt/jellyfin-data sr0 11:0 1 3G 0 rom Jellyfin Installation # From Jellyfin\u0026rsquo;s installation page for Debian: https://jellyfin.org/downloads/server\ncurl -s https://repo.jellyfin.org/install-debuntu.sh | sudo bash\nAfter waiting for installation to finish!\u0026hellip;\nContinue with login, set admin credentials, etc\nCreate media library for Music, keep all default settings except for changing language and region. Add folder /mnt/media/audio (not shown in screenshot).\nTroubleshooting Notes # Trouble when copying data to the mounted drive via SCP:\nscp -i \u0026lt;ssh_key_path\u0026gt; -r /media/localuser/full_backup/audio/ user@192.168.100.11:/mnt/media\nEnter passphrase for key \u0026#39;\u0026lt;ssh_key_path\u0026gt;\u0026#39;: scp: stat remote: No such file or directory scp: failed to upload directory /media/exis/full_backup/audio to /mnt/media Current owner is probably root, not user, which is why there are permission issues copying as user: Check this:\nls -ld /mnt/media mount | grep media Change owner to fix:\nsudo chown user:user /mnt/media sudo chmod 755 /mnt/media ","date":"16 August 2025","externalUrl":null,"permalink":"/homelab-v1/jellyfin/","section":"Homelab-V1s","summary":"","title":"Jellyfin","type":"homelab-v1"},{"content":" Best Practices in Backup Systems: the 3-2-1 Rule # The 3-2-1 Rule is a common guideline for ensuring resilience in backup systems \u0026ndash; that is, the ability to recover from partial failures without losing data. https://www.veeam.com/blog/321-backup-rule.html\nTo summarize:\nKeep at least 3 copies of your data. If 1-2 of them are compromised, you still have another to restore from. Use 2 different media types (local storage, cloud, disk, tape, etc). Different media types have different failure modes, so using more than one reduces the chance of simultaneous loss. At least 1 backup should be offsite. Aim for geographic and network separation to prevent a single disaster from compromising all of your backups. Utilize cloud storage or replicate your backups to another server that you keep at a friend or relative\u0026rsquo;s house, for example. Currently, much of my data lives solely on my laptop\u0026rsquo;s internal SSD\u0026hellip; not ideal. Let\u0026rsquo;s fix that.\nIn my homelab server, my only storage is the 2TB SSD, which doesn\u0026rsquo;t provide any redundancy to prevent data loss in the event of a system/drive failure or a disaster like my house burning down\u0026hellip;\nWhat I\u0026rsquo;m Doing # I decided on backing up to an external HDD to keep as an on-prem, offline backup as well as automating encrypted cloud backups to Backblaze B2.\nI already have a 2TB external HDD and 500GB external SSD that I could use for my offline backup, although I know USB isn\u0026rsquo;t ideal either. Additional drives attached via SATA would offer better reliability and performance\u0026hellip; I will figure this part out later.\nIn very unfortunate foreshadowing, I would later back up data to the 2TB Seagate drive only to have it fail when trying to restore.\nWhat I Could Do Instead # Here are a few other options I considered. Ultimately, I want to keep a small footprint and minimize costs. After all, this is for personal use and experimentation \u0026ndash; I don\u0026rsquo;t need 5 9\u0026rsquo;s of uptime to appease my stakeholders (yet)!\nBuy or build a NAS, configured with RAID for improving redundancy and performance Obtaining a NAS: Synology has a strong reputation for quality and ease of use, but I’m wary of their vendor lock-in \u0026ndash; they restrict certain third-party HDDs, which could limit future upgrades or repairs. Ugreen Build your own by adding multiple HDDs to a second-hand workstation or even a raspberry pi. Then install TrueNAS Buy an Uninterruptible Power Supply (UPS) for power resiliency This is meant to keep the on-prem server/NAS running temporarily in the event of a power outage, at least giving enough time for it to shut down gracefully. For critical systems, use a UPS in conjunction with a generator for better redundancy in extended outages. Install a NAS at a friend or relative\u0026rsquo;s house for off-site backups Use Tailscale for secure remote access Encrypt data in transit and at rest \u0026ndash; even if you trust your friend, you can\u0026rsquo;t guarantee their home\u0026rsquo;s network and physical security from threats like ransomware and physical intruders. Use smart PDU, or configure WoL to only power on the system when needed for pushing backups. This system helps to reduce wasted electricity for your hosting party, but it also reduces risk of getting ransomwared if their network is compromised, since this also functions as an offline backup. Store a standalone, offline disk off-site, again, potentially at a friend or relative\u0026rsquo;s house Encrypt your backup data as you won\u0026rsquo;t be able to ensure its physical security once it leaves your possession Store in a safe-deposit box at a post office or bank For a stealthier approach, store your backup on a microSD card which is easier to conceal\u0026hellip; even put it in a hollow nickel if you want. AWS Deep Glacier for cloud backups Deep Glacier is supposed to be even cheaper than Backblaze B2 for data archival, at just $1/TB/Month; anecdotally I\u0026rsquo;ve read that these savings come at the cost of a painful restore process with slow data retrieval and a more complicated fee structure than backblaze. Since my on-prem backups aren\u0026rsquo;t as robust and it\u0026rsquo;s definitely possible that I\u0026rsquo;ll need to restore from cloud at some point, Backblaze makes more sense to me for now. Use Veame to backup to S3-compatible storage like AWS Glacier or Backblaze B2 Requires license, not available with the community edition. https://www.backblaze.com/blog/how-to-back-up-veeam-to-the-cloud/ Implementation (WIP) # Discover where my virtual disks are, from the host: qm config \u0026lt;VMID\u0026gt;\nWe have:\nvm-101-disk-1 vm-102-disk-1 We can also list all logical volumes with lvs:\nI realized that if I backup these volumes entirely as disk images, I\u0026rsquo;ll also be wasting storage by backing up the empty space in the volumes. I want to have a centralized, single backup job from the proxmox host, without wasting space. In other words, I want to do a filesystem backup rather than backing up the raw disk images.\nMake a bucket, application key for read/write access, and append the following into /root/.bashrc, filling in your information:\nexport B2_REPO_BASE=\u0026#34;...\u0026#34; export B2_ACCOUNT_ID=\u0026#34;...\u0026#34; export B2_ACCOUNT_KEY=\u0026#34;...\u0026#34; export RESTIC_PASSWORD=\u0026#34;...\u0026#34; Run source /root/.bashrc to reload the environment.\nInitialize bucket with folders: restic init --repo b2:your-bucket-name:immich restic init --repo b2:your-bucket-name:jellyfin\nConfirm these commands run succesffully and the folders are initialized in Backblaze B2:\nWe\u0026rsquo;ll reference these environment variables in the backup script.\n/root/backup.sh\n#!/bin/bash set -euo pipefail # Where backups go IMMICH_REPO=\u0026#34;b2:${B2_REPO_BASE}:immich\u0026#34; JELLYFIN_REPO=\u0026#34;b2:${B2_REPO_BASE}:jellyfin\u0026#34; # VM IDs IMMICH_VM=101 JELLYFIN_VM=102 # Logging LOGFILE=\u0026#34;/var/log/proxmox-backup.log\u0026#34; exec \u0026gt; \u0026gt;(tee -a \u0026#34;$LOGFILE\u0026#34;) 2\u0026gt;\u0026amp;1 timestamp() { date +\u0026#34;[%Y-%m-%d %H:%M:%S]\u0026#34; } backup_vm_disk() { local VMID=$1 local DISK=$2 local REPO=$3 echo \u0026#34;$(timestamp) Stopping VM $VMID...\u0026#34; qm stop \u0026#34;$VMID\u0026#34; SNAP_NAME=\u0026#34;backup-$(date +%s)\u0026#34; DISK_PATH=$(qm config \u0026#34;$VMID\u0026#34; | awk -v disk=\u0026#34;$DISK\u0026#34; \u0026#39;$1 == disk {print $2}\u0026#39; | cut -d\u0026#39;,\u0026#39; -f1) echo \u0026#34;$(timestamp) Creating LVM snapshot...\u0026#34; lvcreate --size 5G --snapshot --name \u0026#34;${SNAP_NAME}\u0026#34; \u0026#34;$DISK_PATH\u0026#34; SNAP_PATH=\u0026#34;/dev/$(dirname \u0026#34;$DISK_PATH\u0026#34;)/${SNAP_NAME}\u0026#34; echo \u0026#34;$(timestamp) Mounting snapshot...\u0026#34; mkdir -p /mnt/backup mount \u0026#34;$SNAP_PATH\u0026#34; /mnt/backup echo \u0026#34;$(timestamp) Running restic backup to $REPO...\u0026#34; restic -r \u0026#34;$REPO\u0026#34; backup /mnt/backup echo \u0026#34;$(timestamp) Cleaning up...\u0026#34; umount /mnt/backup lvremove -f \u0026#34;$SNAP_PATH\u0026#34; echo \u0026#34;$(timestamp) Restarting VM $VMID...\u0026#34; qm start \u0026#34;$VMID\u0026#34; echo \u0026#34;$(timestamp) Finished backup for VM $VMID\u0026#34; } echo \u0026#34;$(timestamp) Starting backups...\u0026#34; backup_vm_disk $IMMICH_VM vm-101-disk-1 $IMMICH_REPO backup_vm_disk $JELLYFIN_VM vm-102-disk-1 $JELLYFIN_REPO echo \u0026#34;$(timestamp) Running forget/prune...\u0026#34; restic -r \u0026#34;$IMMICH_REPO\u0026#34; forget --keep-last 3 --prune restic -r \u0026#34;$JELLYFIN_REPO\u0026#34; forget --keep-last 3 --prune echo \u0026#34;$(timestamp) Backups complete.\u0026#34; ","date":"2 August 2025","externalUrl":null,"permalink":"/homelab-v1/backups/","section":"Homelab-V1s","summary":"","title":"Backups","type":"homelab-v1"},{"content":" Immich VM Setup # I\u0026rsquo;ll make another Ubuntu server for my immich server. We\u0026rsquo;ll create and attach a drive for data storage, separate from the OS and application.\nHomelab Subnet # On proxmox host root: Add to /etc/network/interfaces:\nauto vmbr1 iface vmbr1 inet static address 192.168.100.1 netmask 255.255.255.0 bridge_ports none bridge_stp off bridge_fd 0 systemctl restart networking\nThis will be my homelab subnet\nNow attach VMs to vmbr1 in the web UI.\nIn each VM, assign a static IP In default /etc/netplan file (50-cloud-init.yaml).\nnetwork: version: 2 ethernets: ens18: dhcp4: no addresses: - 192.168.100.10/24 routes - to: default via: 192.168.100.1 nameservers: addresses: - 1.1.1.1 - 8.8.8.8 sudo netplan try and then sudo netplan apply\nEdit /etc/sysctl.conf on proxmox host and uncomment or add: net.ipv4.ip_forward=1\nsudo sysctl -p\nsudo iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o vmbr0 -j MASQUERADE\nNow VM can connect to the internet using the host (192.168.100.1) as a NAT gateway.\nSetup Tailscale as gateway to homelab subnet vmbr1 # Change tailscale VM\u0026rsquo;s network to vmbr1 in proxmox.\nNetplan config\nnetwork: version: 2 ethernets: ens18: dhcp4: no addresses: - 192.168.100.2/24 routes - to: default via: 192.168.100.1 nameservers: addresses: - 1.1.1.1 - 8.8.8.8 echo \u0026#34;net.ipv4.ip_forward=1\u0026#34; | sudo tee -a /etc/sysctl.conf sudo sysctl -p sudo iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o tailscale0 -j MASQUERADE\nNow able to ping 8.8.8.8 and local VMs on the subnet like 192.168.100.10 (immich).\nNow to check whether tailscale can be used to reach the local subnet. I\u0026rsquo;ll disable WiFi on my phone to ensure I\u0026rsquo;m not on the same LAN in any capacity to start with, enable tailscale, and try to reach a temporary web server on my immich machine.\nForgot to advertise the subnet on tailscale sudo tailscale up --advertise-routes=192.168.100.0/24 --accept-routes\nApprove in web\n(Machines \u0026gt; machine \u0026gt; Subnets \u0026gt; Review)\nFINALLY!! Tailscale is working together with my VM subnet.\nImmich Installation # Media Storage Setup # Under my immich server\u0026rsquo;s hardware, I\u0026rsquo;ll add a 300GB drive for storing all application data and photos.\nVerify it\u0026rsquo;s attached with lsblk.\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 40G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 38G 0 part └─ubuntu--vg-ubuntu--lv 252:0 0 19G 0 lvm / sdb 8:16 0 300G 0 disk sr0 11:0 1 3G 0 rom user@immich:~$ sudo fdisk /dev/sdb Partition it with sudo fdisk /dev/sdb and format/initialize the filesystem with sudo mkfs.ext4 /dev/sdb1.\nWelcome to fdisk (util-linux 2.39.3). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table. Created a new DOS (MBR) disklabel with disk identifier 0xe9197cc2. Command (m for help): n Partition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions) Select (default p): p Partition number (1-4, default 1): First sector (2048-629145599, default 2048): Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-629145599, default 629145599): Created a new partition 1 of type \u0026#39;Linux\u0026#39; and of size 300 GiB. Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. user@immich:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 40G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 38G 0 part └─ubuntu--vg-ubuntu--lv 252:0 0 19G 0 lvm / sdb 8:16 0 300G 0 disk └─sdb1 8:17 0 300G 0 part sr0 11:0 1 3G 0 rom user@immich:~$ sudo mkfs.ext4 /dev/sdb1 mke2fs 1.47.0 (5-Feb-2023) Discarding device blocks: done Creating filesystem with 78642944 4k blocks and 19660800 inodes Filesystem UUID: 79b8c12e-388b-4e2a-8516-9898c5d6d012 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done Make a mount directory, mount it, and check available space.\nsudo mkdir -p /mnt/immich-data sudo mount /dev/sdb1 /mnt/immich-data df -h /mnt/immich-data Filesystem Size Used Avail Use% Mounted on /dev/sdb1 295G 28K 280G 1% /mnt/immich-data Next, we\u0026rsquo;ll want to create an entry in /etc/fstab to mount this volume when the VM reboots.\nGet the UUID sudo blkid /dev/sdb1\n/dev/sdb1: UUID=\u0026#34;79b8c12e-388b-4e2a-8516-9898c5d6d012\u0026#34; BLOCK_SIZE=\u0026#34;4096\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;e9197cc2-01\u0026#34; Append into /etc/fstab: UUID=79b8c12e-388b-4e2a-8516-9898c5d6d012 /mnt/immich-data ext4 defaults 0 2\nUnmount/remount\nsudo umount /mnt/immich-data sudo systemctl daemon-reload sudo mount -a Ensure it shows mounted in the correct place after issuing lsblk again.\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 40G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 38G 0 part └─ubuntu--vg-ubuntu--lv 252:0 0 19G 0 lvm / sdb 8:16 0 300G 0 disk └─sdb1 8:17 0 300G 0 part /mnt/immich-data sr0 11:0 1 3G 0 rom Now we have a 300GB ext4 volume on /mnt/immich-data that will persist server reboots!!\nImmich Installation # Follow docker instructions to get the latest version available for Ubuntu\u0026hellip; the apt repos are a bit outdated for Immich\u0026rsquo;s provided docker-compose configuration. https://docs.docker.com/engine/install/ubuntu/\nfor pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n# Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nTest installation sudo docker run hello-world\nFollow instructions on https://immich.app/docs/install/docker-compose\nI\u0026rsquo;ll update my .env to make the paths go to my external disk mounted at /mnt/immich-data.\nAfter following the docker install instructions I can reach it in my browser! Awesome.\nThe next screens allow us to set an admin email and password, and get started with basic configuration.\nNGINX Proxy Manager + Cloudflare DNS-01 Validation for Internal SSL Certificates # (Outdated, now reworked in Centralized Reverse Proxy) # References # https://youtu.be/Y7Z-RnM77tA?si=CsAv2LXH85OsSLS6\n","date":"27 July 2025","externalUrl":null,"permalink":"/homelab-v1/immich-installation/","section":"Homelab-V1s","summary":"","title":"Immich Installation","type":"homelab-v1"},{"content":" Create a new email # Create a dedicated email or inbox for various homelab purposes.\nRegister a domain # For good OPSEC and privacy, this should be something unique from other public identities. Although it\u0026rsquo;s typically included, make sure your registrar and domain will include WHOIS privacy, which results in the registrar replacing your private information with theirs in the public database. While\nI installed Proxmox previously but wasn\u0026rsquo;t ready use it until now, so first I changed my email for alerts.\nUpdate Packages # https://pve.proxmox.com/wiki/Package_Repositories\n/etc/apt/sources.list\ndeb http://ftp.debian.org/debian bookworm main contrib deb http://ftp.debian.org/debian bookworm-updates main contrib # Proxmox VE pve-no-subscription repository provided by proxmox.com, # NOT recommended for production use deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription # security updates deb http://security.debian.org/debian-security bookworm-security main contrib apt update \u0026amp;\u0026amp; apt upgrade\nSSH Key # Create a GitHub account using the lab email from earlier Add a passkey via Yubikey Generate a new SSH key for the lab, add it to GitHub Tailscale (VPN for Remote Access) # Create an Ubuntu 24.04 VM named tailscale\nCopy SSH key from Github during installation\nFollow installation on tailscale VM: https://tailscale.com/download/linux\nDownload app and follow installation on mobile.\nCreate test index.html on the tailscale server, serve it with python3 -m http.server 8080. Ensure able to reach while off WiFi, only accessing via Tailscale.\nInstall on workstation by installing tailscale and signing in with same GitHub account. Next up is Immich Installation.\n","date":"19 July 2025","externalUrl":null,"permalink":"/homelab-v1/infrastructure/","section":"Homelab-V1s","summary":"","title":"Infrastructure","type":"homelab-v1"},{"content":"I bought this Dell Optiplex 7040 from my University\u0026rsquo;s surplus store and replaced the RAM and storage (originally 16GB RAM and 256GB SSD).\nhttps://dl.dell.com/topicspdf/optiplex-7040-desktop_owners-manual_en-us.pdf\nDell Optiplex - $60\n64GB Ram https://a.co/d/iPBaAJC - $44.99 * 2 = $89.98 2TB NVMe SSD https://a.co/d/7EjigTP - $109.99 8-port unmanaged Ethernet switch https://a.co/d/6lgx1pb - $17.99 Total: $277.96\n","date":"18 July 2025","externalUrl":null,"permalink":"/homelab-v1/hardware/","section":"Homelab-V1s","summary":"","title":"Hardware","type":"homelab-v1"},{"content":" Summary # This iteration was functional, but pretty basic. Although I used it for a few months from August to December, I was manually creating and configuring my infrastructure and applications.\nInspiration for next time # Document network architecture and IPAM better from the start Dashboard / uptime monitoring Redo public documentation style \u0026ndash; rather than a walkthrough, just showcase architecture and how things are. Walkthroughs and devlogs, etc, can be saved for a separate blog post. Implement proper internal DNS (Pihole or BIND9) Leverage more Infrastructure as Code! Nix/NixOS Terraform Ansible Automate with n8n Figure out how to make Kubernetes make sense for the homelab Run a mail server Run an Anki Sync server ","date":"18 July 2025","externalUrl":null,"permalink":"/homelab-v1/homelab-v1/","section":"Homelab-V1s","summary":"","title":"Lab Retrospective","type":"homelab-v1"},{"content":"Hi! I\u0026rsquo;m Cameron, a cybersecurity engineer based in San Francisco, California.\nI graduated with a B.A. in Computer Science from UC Santa Cruz in December 2024, have earned multiple cybersecurity certifications, and am currently working in system administration and information security.\nPlease feel free to contact me on LinkedIn or by email at hello[at]cameroncandau[dot]com.\n","externalUrl":null,"permalink":"/about/","section":"Cameron Candau","summary":"","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"I publish write-ups for cybersecurity CTF challenges (primarily focused on penetration testing) at https://veilcat.dev.\n","externalUrl":null,"permalink":"/ctf/","section":"Cameron Candau","summary":"","title":"Capture the Flag","type":"page"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"I maintain a homelab to gain experience with new technologies while reclaiming digital agency in my life.\nv2 # Infrastructure-as-Code implementation of the foundation established in v1, followed by scaling to more services. In active development.\nv1 # My first homelab iteration - Dell Optiplex running Proxmox with Immich, Jellyfin, and basic infrastructure. Manual provisioning and configuration of 3 services and a reverse proxy. Incremental backups to cloud.\n","externalUrl":null,"permalink":"/homelab/","section":"Cameron Candau","summary":"","title":"Homelab","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]